import asyncio
from telethon.sync import TelegramClient
import pandas as pd
from konlpy.tag import Okt
from collections import Counter
import sqlite3
import schedule
import time
from datetime import datetime, timedelta
import streamlit as st
from transformers import pipeline
import plotly.express as px
import re

# í…”ë ˆê·¸ë¨ ì„¤ì •
api_id = "16409037"  # my.telegram.orgì—ì„œ ë°œê¸‰ë°›ì€ ID
api_hash = "0a22b7402b242338fcc9f9f904a91e5f"  # my.telegram.orgì—ì„œ ë°œê¸‰ë°›ì€ Hash
phone = "+821093343869"  # ì „í™”ë²ˆí˜¸ (+8210xxxx)
channels = ["@econostudy", "@pengmeup", "@daegurr"]

# ë°ì´í„° ì¤€ë¹„
stocks = pd.read_csv("stocks.csv")
stock_names = stocks["name"].tolist()
stock_codes = stocks["code"].tolist()
industries = open("industries.txt", encoding="utf-8").read().splitlines()

# í•œêµ­ì–´ NLP
okt = Okt()

# ê°ì • ë¶„ì„ ëª¨ë¸
sentiment_analyzer = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")

# SQLite ë°ì´í„°ë² ì´ìŠ¤
conn = sqlite3.connect("stock_mentions.db")
cursor = conn.cursor()
cursor.execute("""
    CREATE TABLE IF NOT EXISTS mentions (
        date TEXT,
        channel TEXT,
        stock TEXT,
        industry TEXT,
        count INTEGER,
        sentiment REAL
    )
""")
conn.commit()

async def scrape_channels(initial_days=3):
    async with TelegramClient("session", api_id, api_hash) as client:
        end_time = datetime.now().replace(hour=0, minute=0, second=0, microsecond=0)
        start_time = end_time - timedelta(days=initial_days)
        
        for channel in channels:
            stock_counts = Counter()
            industry_counts = Counter()
            sentiments = {}
            
            async for message in client.iter_messages(channel, limit=1000):
                if not message.text or message.date < start_time or message.date >= end_time:
                    continue
                
                text = message.text
                msg_date = message.date.strftime("%Y-%m-%d")
                
                # ì¢…ëª© ë§¤ì¹­
                for name, code in zip(stock_names, stock_codes):
                    if name in text or code in text:
                        stock_counts[(msg_date, name)] += 1
                        try:
                            sentiment = sentiment_analyzer(text[:512])[0]
                            score = sentiment["score"] if sentiment["label"].startswith("positive") else -sentiment["score"]
                            sentiments[(msg_date, name)] = sentiments.get((msg_date, name), []) + [score]
                        except:
                            pass
                
                # ì‚°ì—… ë§¤ì¹­
                for industry in industries:
                    if industry in text:
                        industry_counts[(msg_date, industry)] += 1
                
                # ì¶”ê°€ NLP
                nouns = okt.nouns(text)
                for noun in nouns:
                    if noun in stock_names:
                        stock_counts[(msg_date, noun)] += 1
                    if noun in industries:
                        industry_counts[(msg_date, noun)] += 1
            
            # ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥
            for (date, stock), count in stock_counts.items():
                avg_sentiment = sum(sentiments.get((date, stock), [0])) / len(sentiments.get((date, stock), [1]))
                cursor.execute(
                    "INSERT INTO mentions (date, channel, stock, industry, count, sentiment) VALUES (?, ?, ?, ?, ?, ?)",
                    (date, channel, stock, None, count, avg_sentiment)
                )
            for (date, industry), count in industry_counts.items():
                cursor.execute(
                    "INSERT INTO mentions (date, channel, stock, industry, count, sentiment) VALUES (?, ?, ?, ?, ?, ?)",
                    (date, channel, None, industry, count, 0)
                )
            conn.commit()

def run_scraper():
    asyncio.run(scrape_channels(initial_days=1))  # ì´í›„ í•˜ë£¨ ì¹˜
    print("Scraping completed for", datetime.now().strftime("%Y-%m-%d"))

def init_scrape():
    asyncio.run(scrape_channels(initial_days=3))  # ì´ˆê¸° 3ì¼ ì¹˜
    print("Initial 3-day scraping completed")

# ìŠ¤ì¼€ì¤„ë§
schedule.every().day.at("02:00").do(run_scraper)

# Streamlit ëŒ€ì‹œë³´ë“œ
def load_data():
    df = pd.read_sql("SELECT * FROM mentions", conn)
    return df

def main():
    st.title("ì£¼ì‹ ì¡°ê¸°ê²½ë³´ê¸° ëŒ€ì‹œë³´ë“œ")
    
    df = load_data()
    if df.empty:
        st.write("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í¬ë˜í•‘ì„ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
        if st.button("ì´ˆê¸° 3ì¼ ì¹˜ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰"):
            init_scrape()
        return
    
    # í•„í„°
    st.sidebar.header("í•„í„°")
    date = st.sidebar.selectbox("ë‚ ì§œ ì„ íƒ", sorted(df["date"].unique(), reverse=True))
    channel = st.sidebar.multiselect("ì±„ë„ ì„ íƒ", channels, default=channels)
    df_filtered = df[(df["date"] == date) & (df["channel"].isin(channel))]
    
    # ì¢…ëª© ë¶„ì„
    st.header("ì¢…ëª© ì–¸ê¸‰ ë¹ˆë„")
    stock_df = df_filtered[df_filtered["stock"].notnull()][["stock", "count", "sentiment"]]
    if not stock_df.empty:
        stock_df = stock_df.groupby("stock").agg({"count": "sum", "sentiment": "mean"}).reset_index()
        top_stocks = stock_df.sort_values("count", ascending=False).head(10)
        
        fig = px.bar(top_stocks, x="stock", y="count", title="ìƒìœ„ 10ê°œ ì¢…ëª©")
        st.plotly_chart(fig)
        
        # ê°ì • ë¶„ì„
        st.header("ì¢…ëª© ê°ì • ë¶„ì„")
        top_stocks["sentiment_label"] = top_stocks["sentiment"].apply(lambda x: "ê¸ì •" if x > 0 else "ë¶€ì •")
        fig = px.pie(top_stocks, names="stock", values="sentiment", title="ì¢…ëª©ë³„ ê°ì • ë¹„ìœ¨")
        st.plotly_chart(fig)
    
    # ì‚°ì—… ë¶„ì„
    st.header("ì‚°ì—… ì–¸ê¸‰ ë¹ˆë„")
    industry_df = df_filtered[df_filtered["industry"].notnull()][["industry", "count"]]
    if not industry_df.empty:
        industry_df = industry_df.groupby("industry").sum().reset_index()
        top_industries = industry_df.sort_values("count", ascending=False).head(10)
        
        fig = px.bar(top_industries, x="industry", y="count", title="ìƒìœ„ 10ê°œ ì‚°ì—…")
        st.plotly_chart(fig)
    
    # íŠ¸ë Œë“œ ë¶„ì„
    st.header("íŠ¸ë Œë“œ ë¶„ì„ (ìµœê·¼ 3ì¼)")
    last_3_days = df[df["date"].isin(sorted(df["date"].unique(), reverse=True)[:3])]
    if not last_3_days.empty:
        stock_trend = last_3_days[last_3_days["stock"].notnull()].groupby(["date", "stock"])["count"].sum().unstack().fillna(0)
        fig = px.line(stock_trend, title="ì¢…ëª© ì–¸ê¸‰ ì¶”ì´")
        st.plotly_chart(fig)
        
        industry_trend = last_3_days[last_3_days["industry"].notnull()].groupby(["date", "industry"])["count"].sum().unstack().fillna(0)
        fig = px.line(industry_trend, title="ì‚°ì—… ì–¸ê¸‰ ì¶”ì´")
        st.plotly_chart(fig)
    
    # ì•Œë¦¼ ì„¹ì…˜
    st.header("ê¸‰ë“± ì•Œë¦¼")
    prev_day = (pd.to_datetime(date) - timedelta(days=1)).strftime("%Y-%m-%d")
    prev_df = df[df["date"] == prev_day]
    for stock in stock_df["stock"]:
        curr_count = stock_df[stock_df["stock"] == stock]["count"].iloc[0]
        prev_count = prev_df[prev_df["stock"] == stock]["count"].sum()
        if prev_count > 0 and curr_count / prev_count > 2:
            st.write(f"ğŸš¨ {stock}: ì–¸ê¸‰ {curr_count}íšŒ (ì „ì¼ ëŒ€ë¹„ {curr_count/prev_count:.1%} ì¦ê°€)")

if __name__ == "__main__":
    # ìŠ¤ì¼€ì¤„ëŸ¬ ë°±ê·¸ë¼ìš´ë“œ
    import threading
    def run_schedule():
        while True:
            schedule.run_pending()
            time.sleep(60)
    threading.Thread(target=run_schedule, daemon=True).start()
    
    # Streamlit ì‹¤í–‰
    main()